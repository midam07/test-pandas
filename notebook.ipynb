{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "A superstore is planning for the year-end sale. They want to launch a new offer - gold membership, that gives a 20% discount on all purchases, for only $499 which is $999 on other days. It will be valid only for existing customers and the campaign through phone calls is currently being planned for them. The management feels that the best way to reduce the cost of the campaign is to make a predictive model which will classify customers who might purchase the offer.\n",
    "\n",
    "## Objective\n",
    "The superstore wants to predict the likelihood of the customer giving a positive response and wants to identify the different factors which affect the customer's response. You need to analyze the data provided to identify these factors and then build a prediction model to predict the probability of a customer will give a positive response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "This data was gathered during last year's campaign. The data description is as follows:\n",
    "\n",
    "- **Response (target)**: 1 if customer accepted the offer in the last campaign, 0 otherwise\n",
    "- **ID**: Unique ID of each customer\n",
    "- **Year_Birth**: Age of the customer\n",
    "- **Complain**: 1 if the customer complained in the last 2 years\n",
    "- **Dt_Customer**: Date of customer's enrollment with the company\n",
    "- **Education**: Customer's level of education\n",
    "- **Marital**: Customer's marital status\n",
    "- **Kidhome**: Number of small children in customer's household\n",
    "- **Teenhome**: Number of teenagers in customer's household\n",
    "- **Income**: Customer's yearly household income\n",
    "- **MntFishProducts**: The amount spent on fish products in the last 2 years\n",
    "- **MntMeatProducts**: The amount spent on meat products in the last 2 years\n",
    "- **MntFruits**: The amount spent on fruits products in the last 2 years\n",
    "- **MntSweetProducts**: Amount spent on sweet products in the last 2 years\n",
    "- **MntWines**: The amount spent on wine products in the last 2 years\n",
    "- **MntGoldProds**: The amount spent on gold products in the last 2 years\n",
    "- **NumDealsPurchases**: Number of purchases made with discount\n",
    "- **NumCatalogPurchases**: Number of purchases made using catalog (buying goods to be shipped through the mail)\n",
    "- **NumStorePurchases**: Number of purchases made directly in stores\n",
    "- **NumWebPurchases**: Number of purchases made through the company's website\n",
    "- **NumWebVisitsMonth**: Number of visits to company's website in the last month\n",
    "- **Recency**: Number of days since the last purchase\n",
    "\n",
    "## Goals of the Analysis\n",
    "1. **Identify Factors Influencing Customer Response**: Analyze the data to identify the key factors that influence whether a customer accepts the offer.\n",
    "2. **Build a Predictive Model**: Develop a model to predict the likelihood of a customer accepting the offer, which will help in targeting the right customers and reducing campaign costs.\n",
    "3. **Optimize Marketing Strategy**: Use insights from the analysis to optimize the marketing strategy and improve the effectiveness of the campaign.\n",
    "\n",
    "By understanding the data and identifying the important factors, we can better predict customer behavior and tailor the marketing efforts to maximize the success of the year-end sale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "\n",
    "## Overview of the Dataset\n",
    "In this section, we will explore the dataset to understand its structure and contents. This will involve loading the data, displaying basic information, and performing some initial visualizations.\n",
    "\n",
    "### 1. Load the Dataset\n",
    "First, let's load the dataset and take a look at the first few rows to get an idea of what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('superstore_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Structure and Types\n",
    "\n",
    "We will now check the structure of the dataset, including the number of rows and columns, and the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summary Statistics\n",
    "\n",
    "Next, let's generate summary statistics for the numerical columns in the dataset to understand their distribution and central tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Missing Values\n",
    "\n",
    "We need to check for any missing values in the dataset, as handling missing data will be an important step in data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there's missing value in income column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data Distribution\n",
    "To better understand the data, let's visualize the distribution of key variables. This includes both numerical and categorical variables.\n",
    "\n",
    "#### 5.1 Distribution of Numerical Variables\n",
    "We will plot histograms for the numerical variables to see their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plot histograms for numerical variables\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhist(bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m15\u001b[39m))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot histograms for numerical variables\n",
    "df.hist(bins=20, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Box Plot of Numerical Variables\n",
    "\n",
    "We will plot box plots for the numerical variables to identify outliers and understand the spread of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for numerical variables\n",
    "numerical_columns = ['Income']\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "df[numerical_columns].boxplot(rot=45)\n",
    "plt.title('Box Plot of Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for numerical variables\n",
    "numerical_columns = ['Year_Birth']\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "df[numerical_columns].boxplot(rot=45)\n",
    "plt.title('Box Plot of Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for numerical variables\n",
    "numerical_columns = df.columns[df.dtypes != 'object'].drop(['Income', 'Id', 'Year_Birth'])\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "df[numerical_columns].boxplot(rot=45)\n",
    "plt.title('Box Plot of Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Distribution of Categorical Variables\n",
    "\n",
    "We will use bar plots to visualize the distribution of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar plots for categorical variables\n",
    "categorical_columns = ['Education', 'Complain', 'Response', 'Marital_Status']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(categorical_columns):\n",
    "    df[col].value_counts().plot(kind='bar', ax=axes[i//2, i%2])\n",
    "    axes[i//2, i%2].set_title(f'Distribution of {col}')\n",
    "    axes[i//2, i%2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Top and Bottom Values\n",
    "\n",
    "We will examine the top 10 values for Income and mntmeatproduct the bottom 10 values for Year_Birth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 values for Income\n",
    "top_10_income = df.nlargest(10, 'Income')\n",
    "print(\"Top 10 values for Income:\")\n",
    "print(top_10_income[['Id', 'Income']])\n",
    "\n",
    "# Top 10 values for Income\n",
    "top_10_income = df.nlargest(10, 'MntMeatProducts')\n",
    "print(\"Top 10 values for MntMeatProducts:\")\n",
    "print(top_10_income[['Id', 'MntMeatProducts']])\n",
    "\n",
    "# Bottom 10 values for Year_Birth\n",
    "bottom_10_year_birth = df.nsmallest(10, 'Year_Birth')\n",
    "print(\"Bottom 10 values for Year_Birth:\")\n",
    "print(bottom_10_year_birth[['Id', 'Year_Birth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pair Plot\n",
    "\n",
    "We will use pair plots to visualize the relationships between multiple numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Select a subset of columns for the pair plot\n",
    "pairplot_columns = ['Income', 'MntFishProducts', 'MntMeatProducts', 'MntFruits', 'MntSweetProducts', \n",
    "                    'MntWines', 'MntGoldProds', 'NumDealsPurchases', 'Recency', 'Response']\n",
    "\n",
    "# Plot pair plot\n",
    "sns.pairplot(df[pairplot_columns], hue='Response')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Correlation Analysis\n",
    "\n",
    "We will perform a correlation analysis to identify relationships between numerical variables and the target variable (Response)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Introduction\n",
    "In this section, we will prepare the data for modeling. This includes handling missing values, encoding categorical variables, and normalizing numerical variables. Proper data preparation is crucial for building an effective predictive model.\n",
    "\n",
    "### 1. Handling Missing Values\n",
    "First, we will handle any missing values in the dataset. We will decide whether to fill in missing values or drop the corresponding rows/columns based on the amount and significance of the missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values\n",
    "\n",
    "# Fill missing values (example: fill missing income with median income)\n",
    "df['Income'].fillna(df['Income'].median(), inplace=True)\n",
    "\n",
    "# # If other columns have missing values, we need to decide on the strategy\n",
    "# # For simplicity, we will drop rows with any missing values\n",
    "# df.dropna(inplace=True)\n",
    "\n",
    "# Verify that there are no missing values left\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handling Outliers\n",
    "\n",
    "We will handle outliers in the Year_Birth and Income columns to ensure they do not adversely affect our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where Year_Birth is less than 1920\n",
    "df = df[df['Year_Birth'] >= 1940]\n",
    "\n",
    "# Remove rows where Income is greater than or equal to 400000\n",
    "df = df[df['Income'] < 400000]\n",
    "\n",
    "# Remove rows where MntMeatProducts is greater than or equal to 1000\n",
    "df = df[df['MntMeatProducts'] < 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encoding Categorical Variables\n",
    "\n",
    "We will encode categorical variables using appropriate techniques such as one-hot encoding for nominal variables and label encoding for ordinal variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering\n",
    "\n",
    "We will create new features or transform existing ones to enhance the predictive power of the model.\n",
    "\n",
    "#### 4.1 Creating Age from Year_Birth\n",
    "\n",
    "We will create a new feature Age from Year_Birth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Age feature\n",
    "df['Age'] = 2024 - df['Year_Birth']\n",
    "\n",
    "# Drop the original Year_Birth column\n",
    "df.drop('Year_Birth', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Splitting the Data\n",
    "\n",
    "We will split the data into training and testing sets to evaluate the performance of our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the target variable and features\n",
    "X = df.drop(['Response', 'Id', 'Dt_Customer'], axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## Introduction\n",
    "In this section, we will build a predictive model using the Random Forest Classifier. We will train the model on our training dataset\n",
    "\n",
    "### 1. Training the Model\n",
    "We will start by training the Random Forest Classifier on the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the model\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the = model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Evaluate the = model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter Tuning\n",
    "\n",
    "To improve the performance of our model, we will perform hyperparameter tuning using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [4, 6, 8, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Introduction\n",
    "In this section, we will evaluate the performance of our optimized Random Forest Classifier model. We will use various evaluation metrics such as the confusion matrix, classification report, and accuracy score to assess how well our model performs on the testing dataset.\n",
    "\n",
    "### 1. Confusion Matrix\n",
    "The confusion matrix provides a summary of the prediction results on the testing dataset. It shows the number of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_rfc = grid_search.best_estimator_\n",
    "y_pred_best = best_rfc.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Purchased', 'Purchased'], yticklabels=['Not Purchased', 'Purchased'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classification Report\n",
    "\n",
    "The classification report provides a detailed breakdown of the precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Not Purchased', 'Purchased']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "\n",
    "## Introduction\n",
    "In this section, we will discuss how to deploy our optimized Random Forest Classifier model. Deployment involves saving the trained model so that it can be used to make predictions on new data. We will use joblib to save the model and provide an example of how to load the model and make predictions.\n",
    "\n",
    "### 1. Saving the Model\n",
    "We will use joblib to save the trained model to a file. This allows us to reuse the model without having to retrain it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# # Save the trained model to a file\n",
    "# joblib_file = \"rfc_model.joblib\"\n",
    "# joblib.dump(rfc, joblib_file)\n",
    "\n",
    "# print(f\"Model saved to {joblib_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
